{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab3_DL.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMybvoSBPvZ6",
        "colab_type": "text"
      },
      "source": [
        "## Лабораторная работа №3.\n",
        "\n",
        "Дедлайн: 21 мая 23:59"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sukzY2hyPvZ8",
        "colab_type": "text"
      },
      "source": [
        "### Часть 1. Overfit it (1.5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPyCWAyzPvZ_",
        "colab_type": "text"
      },
      "source": [
        "Будем работать с датасетом [Fashion-MNIST](https://github.com/zalandoresearch/fashion-mnist) (*hint: он доступен в torchvision*).\n",
        "\n",
        "Ваша задача состоит в следующем:\n",
        "1. Обучить сеть, которая покажет >= 0.94 test accuracy.\n",
        "2. Пронаблюдать и продемонстрировать процесс переобучения сети с увеличением числа параметров (==нейронов) и продемонстрировать это наглядно (например, на графиках).\n",
        "3. Попробовать частично справиться с переобучением с помощью подходящих приемов (Dropout/batchnorm/augmentation etc.)\n",
        "\n",
        "*Примечание*: Пункты 2 и 3 взаимосвязаны, в п.3 Вам прелагается сделать полученную в п.2 сеть менее склонной к переобучению. Пункт 1 является независимым от пунктов 2 и 3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FgmeIHmPvaB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import torch, torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysmklmz96ndO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set = torchvision.datasets.FashionMNIST(root='./', train=True, download=True, transform=transforms.ToTensor())\n",
        "trainLoader = torch.utils.data.DataLoader(train_set, batch_size=512, shuffle=True)\n",
        "\n",
        "test_set = torchvision.datasets.FashionMNIST(root='./', train=False, download=True, transform=transforms.ToTensor())\n",
        "testLoader = torch.utils.data.DataLoader(test_set, batch_size=512, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLIMNJyTEquV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = nn.Sequential(nn.Linear(28*28, 1024),\n",
        "                   nn.BatchNorm1d(1024),\n",
        "                   nn.ReLU(),\n",
        "                   nn.Linear(1024, 256),\n",
        "                   nn.BatchNorm1d(256),\n",
        "                   nn.ReLU(),\n",
        "                   nn.Linear(256, 10),\n",
        "                   )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQ0DEER_GUsQ",
        "colab_type": "code",
        "outputId": "d326da5d-64ce-4e4c-f04e-09a9bd7ffe03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        }
      },
      "source": [
        "import time\n",
        "\n",
        "num_epochs = 15\n",
        "lr = 0.1\n",
        "momentum = 0.9\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=momentum)\n",
        "\n",
        "train_loss = []\n",
        "val_accuracy = []\n",
        "\n",
        "start_time= time.time()\n",
        "for epoch in range(num_epochs):\n",
        "  net.train(True)\n",
        "  for i, (images, labels) in enumerate(trainLoader):\n",
        "    images = Variable(images.view(-1, 28*28))\n",
        "    labels = Variable(labels)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    train_loss.append(loss.data.numpy())\n",
        "  \n",
        "  net.train(False)\n",
        "  for i, (images, labels) in enumerate(testLoader):\n",
        "    images = Variable(images.view(-1, 28*28))\n",
        "    #labels = Variable(labels)\n",
        "    output = net(images)\n",
        "    pred = output.max(1)[1].data.numpy()\n",
        "    labels = labels.data.numpy()\n",
        "    val_accuracy.append(np.mean(labels == pred))\n",
        "  \n",
        "  print('Accuracy = {:}'.format(np.mean(val_accuracy[-len(test_set) // 512 :]) * 100))\n",
        "    \n",
        "print(val_accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy = 9.946576286764705\n",
            "Accuracy = 10.05859375\n",
            "Accuracy = 9.955193014705882\n",
            "Accuracy = 10.00689338235294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-79-5f32d99662fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Buz2fwn2PvaI",
        "colab_type": "text"
      },
      "source": [
        "### Часть 2. Almost Shakespeare (1.5 points)\n",
        "\n",
        "В этой части задания мы научимся генерировать текст с помощью нейронных сетей. Конкретнее, обучим нейронную сеть на сонетах Шекспира и попросим нейросеть написать свой сонет.\n",
        "\n",
        "Генерация текста обычно включает в себя следующие шаги:\n",
        "    \n",
        "1. Загрузка данных.\n",
        "2. Создание словарей слов/символов.\n",
        "3. Препроцессинг данных.\n",
        "4. Обучение модели (нейросети).\n",
        "5. Генерация нового текста.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-xXDjf1PvaK",
        "colab_type": "text"
      },
      "source": [
        "#### Часть 1. Предобработка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cLMQmq4PvaM",
        "colab_type": "text"
      },
      "source": [
        "Для начала загрузим данные. Файл с сонетами Шекспира доступен по [ссылке](http://www.gutenberg.org/ebooks/1041?msg=welcome_stranger). Кроме того, он находится рядом с этим ноутбуком (`sonnetes.txt`).\n",
        "\n",
        "Базовая предобработка уже сделана: текст состоит непосредственно из поэм Шекспира и названий/номеров глав, все техническая информация удалена."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqq_ESuWPvaO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('sonnets.txt', 'r') as iofile:\n",
        "    text = iofile.readlines()\n",
        "    \n",
        "TEXT_START = 45\n",
        "TEXT_END = -368\n",
        "text = text[TEXT_START:TEXT_END]\n",
        "assert len(text) == 2616"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i5FJqrsnPvaU",
        "colab_type": "text"
      },
      "source": [
        "Так как в этот раз мы хотим научиться предсказывать текст, понизим сложность задачи и приведем текст к нижнему регистру.\n",
        "\n",
        "В настоящий момент переменная `text` представляет собой список из строк. Объедините все строки в одну и приведите к нижнему регистру. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM8mZkkzPvaW",
        "colab_type": "code",
        "outputId": "70c744dd-09ad-497b-8d6a-84e9255f151c",
        "colab": {}
      },
      "source": [
        "# Объедините все строки в одну и приведите к нижнему регистру.\n",
        "# Результат запишите в переменную text.\n",
        "\n",
        "# Your great code here\n",
        "\n",
        "assert len(text) == 100225, 'Are you sure you have concatenated all the strings?'\n",
        "assert not any([x in set(text) for x in string.ascii_uppercase]), 'Uppercase letters are present'\n",
        "print('Отлично!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Отлично!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3iN4fkvwPvaj",
        "colab_type": "text"
      },
      "source": [
        "Выделите множество всех символов, с которыми нам довелось встретиться в переменную `tokens`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_jtPSpHPval",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokens = sorted(set(text))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6OkBp6XPvas",
        "colab_type": "text"
      },
      "source": [
        "Постройте словарь `token_to_idx` вида <символ>: <индекс> и словарь `idx_to_token` вида <индекс>: <символ>."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GiqZyrUUPvat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# словарь вида <индекс>:<символ>\n",
        "# Your great code here\n",
        "\n",
        "# словарь вида <символ>:<индекс>\n",
        "# Your great code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWbLeHD8Pva0",
        "colab_type": "text"
      },
      "source": [
        "*Комментарий: т.к. у нас всего 38 различных токенов, в этот раз воспользуемся one-hot encoding'ом.*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2r5kOlJPva1",
        "colab_type": "text"
      },
      "source": [
        "## Построение модели."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zRtVjMKJPva3",
        "colab_type": "text"
      },
      "source": [
        "Теперь наша задача - создать и обучить рекуррентную нейронную сеть, которая сможет генерировать что-то похожее на поэзию Шекспира.\n",
        "\n",
        "Для начала воспользуемся классической RNN, аналогичной построенной на семинаре. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_50xdttiPva6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your modified code from class here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBtWfIMnPvbC",
        "colab_type": "text"
      },
      "source": [
        "Постройте график функции потерь в зависимости от номера эпохи."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ML3ZMmmPvbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your plot code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aijnyc4NPvbM",
        "colab_type": "code",
        "outputId": "1b109be0-deea-4bbb-c8a7-0f5cb76111a6",
        "colab": {}
      },
      "source": [
        "# Пример сгенерированного текста. Функция `generate_text` отсутствует в коде выше.\n",
        "# print(generate_text(length=500, temperature=0.2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hide my will in thine?\n",
            "  shall will in of the simend that in my sime the seave the seave the sorll the soren the sange the seall seares and and the fart the wirl the seall the songh whing that thou hall will thoun the soond beare the with that sare the simest me the fart the wirl the songre the with thy seart so for shat so for do the dost the sing the sing the sing the soond canding the sack and the farling the wirl of sore sich and that with the seare the seall so fort the with the past the wirl the simen the wirl the sores the sare\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uoCvkUlPvbT",
        "colab_type": "text"
      },
      "source": [
        "### Более поэтичная модель\n",
        "\n",
        "Теперь давайте воспользуемся LSTM слоем вместо классической RNN и сравним результаты."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xJOr_fAPvbU",
        "colab_type": "text"
      },
      "source": [
        "Снова постройте график функции потерь от числа эпох. Стал ли финальный loss лучше?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LEfdAd1PvbV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your beautiful code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdyOAwJRPvbZ",
        "colab_type": "text"
      },
      "source": [
        "Сгенерируйте текст с помощью обученной сети для различных значений параметра `temperature`: `[0.1, 0.2, 0.5, 1.0, 2.0]` (\"температуры\" при генерации). Оцените результаты визуально, попробуйте их проинтерпретировать."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cmi8DZqPvbb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Text generation with different tempearature values here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWad468SPvbe",
        "colab_type": "text"
      },
      "source": [
        "Здесь можно оставить свои рассуждения касательно интерпретации результатов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvjDUV99Pvbf",
        "colab_type": "text"
      },
      "source": [
        "#### Сохранение и загрузка модели\n",
        "\n",
        "Сохраните обученную модель на диск, затем загрузите ее и сгенерируйте текст. Примеры доступны по [ссылке](https://pytorch.org/tutorials/beginner/saving_loading_models.html])."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uv6aLvfIPvbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving and loading code here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzgLdkiZPvbl",
        "colab_type": "text"
      },
      "source": [
        "Данная часть задания завершена."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGk1VYjgPvbm",
        "colab_type": "text"
      },
      "source": [
        "#### Полезные ссылки\n",
        "1. <a href='http://karpathy.github.io/2015/05/21/rnn-effectiveness/'> Статья Андрея Карпатого про RNN. </a> В качестве примеров рассматриваются задачи генерации Шекспировских текстов, Latex формул, Linux Source Code и детских имен.\n",
        "2. <a href='https://github.com/karpathy/char-rnn'> Репозиторий с кодом по char-rnn </a> (тоже за авторством Андрея Карпатого)\n",
        "3. Полезный репозиторий по PyTorch: [ссылка](https://github.com/spro/practical-pytorch`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTqsklwLPvbn",
        "colab_type": "text"
      },
      "source": [
        "____________________\n",
        "____________________\n",
        "____________________\n",
        "____________________\n",
        "____________________\n",
        "____________________\n",
        "____________________\n",
        "____________________\n",
        "____________________\n",
        "____________________\n",
        "____________________"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9y9uUqHPvbp",
        "colab_type": "text"
      },
      "source": [
        "Части задания 3 (HAR classification) и 4 (kaggle) доступны на выбор. Вы можете выполнить одну из них, или обе."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY4JRj-XPvbq",
        "colab_type": "text"
      },
      "source": [
        "### Часть 3. HAR classification with raw data (2+ points)\n",
        "__Disclaimer__: В данном задании придется экспериментировать, подбирать оптимальную структуру сети для решения задачи и активно искать подскзаки в сети.\n",
        "\n",
        "\n",
        "Данное задание составлено на основе данного [поста](https://burakhimmetoglu.com/2017/08/22/time-series-classification-with-tensorflow/). С помощью вручную сгенерированных фичей и классических подходов задача распознования движений была решена с точностью 96%. \n",
        "\n",
        "Также будет полезным изучить [вот этот](https://github.com/healthDataScience/deep-learning-HAR), а так же [вот этот репозиторий](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition), где к данной задаче рассматривается несколько подходов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqCc6pGLPvbs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import math\n",
        "import pylab\n",
        "import warnings as w\n",
        "import os\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnS0bqM8Pvbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "matplotlib.rcParams.update({'font.size':14})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp-C5nrjPvbz",
        "colab_type": "text"
      },
      "source": [
        "Вернемся к задаче классификации движений на основе [данных](https://archive.ics.uci.edu/ml/datasets/Human+Activity+Recognition+Using+Smartphones) из репозитория UCI ([прямая ссылка на скачивание](https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip)). \n",
        "\n",
        "В этот раз будем работать с исходными, а не предобработанными данными. Данные представляют собой сигналы с гироскопа и акселерометра, закрепленного на теле человека. Каждому семплу соотвествует 9 связанных временных рядов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL07pzt7Pvb0",
        "colab_type": "text"
      },
      "source": [
        "В начале приведена визуализация данных на основе PCA над вручную сгенерированными признаками. Для отрисовки графиков (цвет и легенда) нам также понадобятся метки классов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VIIPpKHFPvb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_with_engineered_features = np.genfromtxt(os.path.join(\"UCI HAR Dataset\", \"train\", \"X_train.txt\"))\n",
        "y_train = np.genfromtxt(os.path.join(\"UCI HAR Dataset\", \"train\", \"y_train.txt\"))\n",
        "\n",
        "y_train_list = list(y_train)\n",
        "X_unique = np.array([X_train_with_engineered_features[y_train_list.index(l)]\n",
        "                             for l in sorted(list(set(y_train)))])\n",
        "\n",
        "legend_labels = [\"WALKING\", \"WALKING.UP\", \"WALKING.DOWN\", \"SITTING\", \"STANDING\", \"LAYING\"]\n",
        "colors_list = ['red', 'blue', 'green', 'orange', 'cyan', 'magenta']\n",
        "mapped_colors = [colors_list[int(i)-1] for i in y_train]\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "\n",
        "X_train_pca = pca.fit_transform(X_train_with_engineered_features)\n",
        "\n",
        "plt.figure(figsize=(15,10))\n",
        "pylab.scatter(X_train_pca[:, 0], X_train_pca[:, 1],\n",
        "             c=mapped_colors)\n",
        "plt.grid()\n",
        "for idx, x in enumerate(pca.transform(X_unique)):\n",
        "    plt.scatter(x[0], \n",
        "                x[1], \n",
        "                c=colors_list[idx], \n",
        "                label=legend_labels[idx])\n",
        "plt.xlabel('First principal component')\n",
        "plt.ylabel('Second principal component')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9C3YZYVPvb4",
        "colab_type": "text"
      },
      "source": [
        "#### Предобработка данных\n",
        "Предобработка сделана за нас автором [данного репозитория](https://github.com/guillaume-chevalier/LSTM-Human-Activity-Recognition). Будьте осторожны с путями."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAYkcp96Pvb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Useful Constants\n",
        "\n",
        "# Those are separate normalised input features for the neural network\n",
        "INPUT_SIGNAL_TYPES = [\n",
        "    \"body_acc_x_\",\n",
        "    \"body_acc_y_\",\n",
        "    \"body_acc_z_\",\n",
        "    \"body_gyro_x_\",\n",
        "    \"body_gyro_y_\",\n",
        "    \"body_gyro_z_\",\n",
        "    \"total_acc_x_\",\n",
        "    \"total_acc_y_\",\n",
        "    \"total_acc_z_\"\n",
        "]\n",
        "\n",
        "# Output classes to learn how to classify\n",
        "LABELS = [\n",
        "    \"WALKING\", \n",
        "    \"WALKING_UPSTAIRS\", \n",
        "    \"WALKING_DOWNSTAIRS\", \n",
        "    \"SITTING\", \n",
        "    \"STANDING\", \n",
        "    \"LAYING\"\n",
        "]\n",
        "\n",
        "DATA_PATH = \"./\"\n",
        "\n",
        "DATASET_PATH = DATA_PATH + \"UCI HAR Dataset/\"\n",
        "print(\"\\n\" + \"Dataset is now located at: \" + DATASET_PATH)\n",
        "\n",
        "TRAIN = \"train/\"\n",
        "TEST = \"test/\"\n",
        "\n",
        "\n",
        "# Load \"X\" (the neural network's training and testing inputs)\n",
        "\n",
        "def load_X(X_signals_paths):\n",
        "    X_signals = []\n",
        "    \n",
        "    for signal_type_path in X_signals_paths:\n",
        "        file = open(signal_type_path, 'r')\n",
        "        # Read dataset from disk, dealing with text files' syntax\n",
        "        X_signals.append(\n",
        "            [np.array(serie, dtype=np.float32) for serie in [\n",
        "                row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "            ]]\n",
        "        )\n",
        "        file.close()\n",
        "    \n",
        "    return np.transpose(np.array(X_signals), (1, 2, 0))\n",
        "\n",
        "X_train_signals_paths = [\n",
        "    os.path.join(*[DATASET_PATH, TRAIN, \"Inertial Signals/\", signal+\"train.txt\"]) for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "X_test_signals_paths = [\n",
        "    os.path.join(*[DATASET_PATH, TEST, \"Inertial Signals/\", signal+\"test.txt\"]) for signal in INPUT_SIGNAL_TYPES\n",
        "]\n",
        "\n",
        "X_train = load_X(X_train_signals_paths)\n",
        "X_test = load_X(X_test_signals_paths)\n",
        "\n",
        "\n",
        "# Load \"y\" (the neural network's training and testing outputs)\n",
        "\n",
        "def load_y(y_path):\n",
        "    file = open(y_path, 'r')\n",
        "    # Read dataset from disk, dealing with text file's syntax\n",
        "    y_ = np.array(\n",
        "        [elem for elem in [\n",
        "            row.replace('  ', ' ').strip().split(' ') for row in file\n",
        "        ]], \n",
        "        dtype=np.int32\n",
        "    )\n",
        "    file.close()\n",
        "    \n",
        "    # Substract 1 to each output class for friendly 0-based indexing \n",
        "    return y_ - 1\n",
        "\n",
        "y_train_path = os.path.join(DATASET_PATH, TRAIN, \"y_train.txt\")\n",
        "y_test_path = os.path.join(DATASET_PATH, TEST, \"y_test.txt\")\n",
        "\n",
        "y_train = load_y(y_train_path)\n",
        "y_test = load_y(y_test_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKBuWoIPPvb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Input Data \n",
        "\n",
        "training_data_count = len(X_train)  # 7352 training series (with 50% overlap between each serie)\n",
        "test_data_count = len(X_test)  # 2947 testing series\n",
        "n_steps = len(X_train[0])  # 128 timesteps per series\n",
        "n_input = len(X_train[0][0])  # 9 input parameters per timestep\n",
        "\n",
        "\n",
        "# LSTM Neural Network's internal structure\n",
        "\n",
        "n_hidden = 32 # Hidden layer num of features\n",
        "n_classes = 6 # Total classes (should go up, or should go down)\n",
        "\n",
        "\n",
        "# Some debugging info\n",
        "\n",
        "print(\"Some useful info to get an insight on dataset's shape and normalisation:\")\n",
        "print(\"(X shape, y shape, every X's mean, every X's standard deviation)\")\n",
        "print(X_test.shape, y_test.shape, np.mean(X_test), np.std(X_test))\n",
        "print(\"The dataset is therefore properly normalised, as expected, but not yet one-hot encoded.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYd6oE5hPvb_",
        "colab_type": "text"
      },
      "source": [
        "#### Построение сети и эксперименты. (100% +)\n",
        "\n",
        "__Ваша задача - построить сеть, которая решит задачу классификации с точностью (`accuracy`) не менее 85%.__\n",
        "Разбалловка следующая:\n",
        "* $=$85% - 2 points\n",
        "* $>=$89% - 2.5 points\n",
        "* $>=$91% - 3 points\n",
        "\n",
        "\n",
        "__Warning!__ В сети существует несколько решений данной задачи с использованием различных фреймворков. При проверке это будет учитываться, так что свое решение нужно будет объяснить. Пожалуйста, не копируйте бездумно код, такие задания будут оценены 0 баллов. Если задача не решается - можете обратиться к заданию по классификации изображений."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I85gLocFPvcA",
        "colab_type": "text"
      },
      "source": [
        "После выполнения задания заполните небольшой отчет об экспериментах вида \"Я пробовал(а) ... подходы и получил(а) ... результаты. Наконец, после N+1 чашки кофе/бессонной ночи у меня получилось, и весь секрет был в ...\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8sX3EZGEPvcB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your experiments here"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xJwvUsrPvcF",
        "colab_type": "text"
      },
      "source": [
        "### Часть 4. Birds classification.\n",
        "На kagge доступно [in-class соревнование](https://www.kaggle.com/c/bird-classification3/overview), проводившееся для студентов ШАДа. \n",
        "\n",
        "Ваше задание: научиться классифицировать изображения птиц. Вы можете обучить сеть с нуля или же воспользоваться fine-tunning'ом. Полезная ссылка на [предобученные модели](https://pytorch.org/docs/stable/torchvision/models.html).\n",
        "Разбаловка следующая:\n",
        "* $=$80% - 2 points\n",
        "* $>=$81.5% - 2.5 points\n",
        "* $>=$82.8% - 3 points"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kNYN1FxPvcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Your code here"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}